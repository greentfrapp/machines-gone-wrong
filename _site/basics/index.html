<!DOCTYPE html>
<html lang="en"><head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta property="og:description" content="Inspired by recent ethical concerns in AI, Machines Gone Wrong is an online guide to help AI practitioners get up to date with socio-technical discussions surrounding AI systems." />
  	<meta property="og:image" content="https://greentfrapp.github.io/project-asimov/assets/frontispiece.jpg" />
	<link rel="shortcut icon" href="https://greentfrapp.github.io/assets/favicon.ico" type="image/x-icon">
	<link rel="icon" href="https://greentfrapp.github.io/assets/favicon.ico" type="image/x-icon"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Getting Started | Machines Gone Wrong</title>
<meta name="generator" content="Jekyll v3.7.4" />
<meta property="og:title" content="Getting Started | Machines Gone Wrong" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/basics/" />
<meta property="og:url" content="http://localhost:4000/basics/" />
<script type="application/ld+json">
{"@type":"WebPage","url":"http://localhost:4000/basics/","headline":"Getting Started | Machines Gone Wrong","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/components/icon.min.css" />
	<link rel="stylesheet" href="/assets/semantic.min.css">
	<link rel="stylesheet" href="/assets/main.css">
	<script src="https://cdnjs.cloudflare.com/ajax/libs/d3/5.9.2/d3.min.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>
	<script src="https://unpkg.com/vue"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/showdown/1.9.0/showdown.min.js"></script>
	<script src="https://unpkg.com/intersection-observer@0.5.1/intersection-observer.js"></script>
	<!-- <script src="https://unpkg.com/scrollama"></script> -->
	<!-- <script src="/assets/guide/scroller.js"></script> -->
	<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/cookieconsent@3/build/cookieconsent.min.css" />
	<script type="text/front-matter">
	title: "Machines Gone Wrong"
	description: "Inspired by recent ethical concerns in AI, Machines Gone Wrong is an online guide to help AI practitioners get up to date with socio-technical discussions surrounding AI systems."
	authors:
	- Lim Swee Kiat: https://greentfrapp.github.io
	affiliations:
	- Singapore University of Technology and Design: https://urbanscience.sutd.edu.sg/
	</script>
</head><body id="guide-body" ><div class="ui sidebar inverted vertical menu light">
  <a class="item chapter" href="/">
    Introduction
  </a>
  <a class="item chapter" href="/basics/">
    Getting Started
  </a>
  <a class="item subsection" href="/basics/#ethics-of-artificial-intelligence-ai-ethics">
    Ethics of Artificial Intelligence
  </a>
  <a class="item subsection" href="/basics/#artificial-intelligence-systems-ais">
    Artificial Intelligence Systems
  </a>
  <a class="item subsection" href="/basics/#what-is-different-about-ai">
    What is different about AI?
  </a>
  <a class="item subsection" href="/basics/#the-most-important-question">
    The Most Important Question
  </a>
  <a class="item chapter" href="/fairness/">
    Algorithmic Bias
  </a>
  <a class="item section" href="/fairness/">
    Understanding Fairness
  </a>
  <a class="item subsection" href="/fairness/#disparate-treatment-disparate-impact">
    Disparate Treatment and Disparate Impact
  </a>
  <a class="item subsection" href="/fairness/#a-fair-fat-pet-predictor">
    A Fair Fat Pet Predictor
  </a>
	<a class="item subsection" href="/fairness/#the-impossibility-theorem">
    The Impossibility Theorem
  </a>
  <a class="item subsection" href="/fairness/#context-free-fairness">
    Context-Free Fairness
  </a>
  <a class="item subsection" href="/fairness/#learning-about-the-context">
    Learning about the Context
  </a>
  <a class="item section" href="/bias_i/">
    Understanding Bias I
  </a>
  <a class="item subsection" href="/bias_i/#two-types-of-harms">
    Two Types of Harms
  </a>
  <a class="item subsection" href="/bias_i/#harms-of-allocation">
    Harms of Allocation
  </a>
  <a class="item subsection" href="/bias_i/#harms-of-representation">
    Harms of Representation
  </a>
  <a class="item section" href="/bias_ii/">
    Understanding Bias II
  </a>
  <a class="item subsection" href="/bias_ii/#bias-from-data">
    Bias from Data
  </a>
  <a class="item subsection" href="/bias_ii/#bias-from-algorithm-design">
    Bias from Algorithm Design
  </a>
  <a class="item subsection" href="/bias_ii/#bias-from-deployment">
    Bias from Deployment
  </a>
  <a class="item section" href="/checklist/">
    Summary Checklist
  </a>
  <a class="item section" href="/resources/">
    Resources
  </a>
  <!-- <a class="item subsection" href="/resources/#tools">
    Tools
  </a>
  <a class="item subsection" href="/resources/#datasets">
    Datasets
  </a>
  <a class="item subsection" href="/resources/#readings">
    Readings
  </a> -->
  <a class="item chapter" href="/about/">
    About
  </a><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <!-- <h2 class="footer-heading"></h2> -->

    <div class="footer-col-wrapper">

      <!-- <div class="footer-col footer-col-2"> --><ul class="social-media-list"><li><a href="https://github.com/greentfrapp"><svg class="svg-icon" viewBox="0 0 16 16"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <!-- <span class="username">greentfrapp</span> --></a></li><li><a href="https://www.linkedin.com/in/sweekiat"><svg class="svg-icon" viewBox="0 0 16 16"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <!-- <span class="username">sweekiat</span> --></a></li></ul>
<!-- </div> -->

      <!-- <div class="footer-col footer-col-3">
        <p></p>
      </div> -->
    </div>

  </div>

</footer>
</div><div id="app" class="pusher light">
		<div id="nav" class="fixed" :class="{light: !darkmode}">
			<div class="logo">
				<i class="icon bars large toggle-menu"></i>
				<div style="width: 140px; display: inline-block; vertical-align: middle;" class="toggle-menu">
	  			<a> 
					Machines Gone Wrong
				</a>
				</div>
				<div class="ui toggle checkbox" id="dark-mode-switch">
				  <input type="checkbox" name="darkmode" v-model="darkmode" @click="toggleDarkmode">
				  <label>Dark Mode</label>
				</div>
			</div>
		</div>
		<div :class="{light: !darkmode}">
			<div class="content guide-content">
				<h1 id="getting-started">Getting Started</h1>

<div class="box-red">
<p class="emph">
  <span class="bit" v-if="showBit">Human-designed</span> AI ethics can be confusing. To practitioners, <span class="bit" v-if="showBit">human-designed</span> AI is kind of just clever mathematics. So how can a bunch of code and equations be ethical or unethical? Why are we so worried about <span class="bit" v-if="showBit">human-designed</span> AI ethics?
</p>
<p>
  This section tries to give a warm-up to <span class="bit" v-if="showBit">human-designed</span> AI ethics before we dive into the deep end. It will cover the following:
</p>

<div class="ui list">
  <div class="item">
    <i class="check circle icon"></i>
    <div class="content">
        What do we mean by <span class="bit" v-if="showBit">human-designed</span> AI ethics?
    </div>
  </div>
  <div class="item">
    <i class="check circle icon"></i>
    <div class="content">
        What do we mean by <span class="bit" v-if="showBit">human-designed</span> AI systems?
    </div>
  </div>
  <div class="item">
    <i class="check circle icon"></i>
    <div class="content">
        How is <span class="bit" v-if="showBit">human-designed</span> AI different from other technologies?
    </div>
  </div>
  <div class="item">
    <i class="check circle icon"></i>
    <div class="content">
        What is the single most important question when implementing <span class="bit" v-if="showBit">human-designed</span> AI solutions?
    </div>
  </div>
</div>

</div>

<hr />

<h2 id="ethics-of-artificial-intelligence-ai-ethics">Ethics of <span class="bit" v-if="showBit">Human-Designed</span> Artificial Intelligence<br />(<span class="bit" v-if="showBit">Human-Designed</span> AI Ethics)</h2>

<blockquote>
  <p>On my view, <em>computer ethics</em> is the analysis of the nature and social impact of computer technology and the corresponding formulation and justification of policies for the ethical use of such technology.</p>
</blockquote>

<p><em>What is Computer Ethics? (Moor, 1985) <dt-cite cite="moor1985computer"></dt-cite></em></p>

<p>Discussions of <span class="bit" v-if="showBit">human-designed</span> AI ethics typically fall into two categories: how people treat <span class="bit" v-if="showBit">human-designed</span> AI (think Chappie and Bicentennial Man) and how <span class="bit" v-if="showBit">human-designed</span> AI treat people (think Terminator and HAL9000).</p>

<h3 id="treatment-of-human-designed-ai-by-humans">Treatment of <span class="bit" v-if="showBit">human-designed</span> AI by Humans</h3>

<div>
<img class="comic" width="380px" src="/assets/comics/andrew.png" title="I don’t know what he feels inside but I don’t know what you feel inside. When you talk to him you’ll find he reacts to the various abstractions as you and I do, and what else counts? If someone else’s reactions are like your own, what more can you ask for?" alt="Siri is not a tiny sprite living in an iPhone." />
</div>

<p>Anyone who has been touched by Robin Williams’s portrayal of Andrew in Bicentennial Man might have thought about the idea of granting rights to robots and <span class="bit" v-if="showBit">human-designed</span> AI systems. In Life 3.0, Max Tegmark recounted a heated discussion between Larry Page and Elon Musk on robot rights <dt-cite cite="tegmark2017life"></dt-cite>.</p>

<blockquote>
  <p>At times, Larry accused Elon of being “specieist”: treating certain life forms as inferior just because they were silicon-based rather than carbon-based.</p>
</blockquote>

<p>Realistically though, <span class="bit" v-if="showBit">human-designed</span> AI systems that require us to rethink notions of humanity and consciousness still remain on the far-flung horizon. Instead, let’s focus on the more urgent issue of how <span class="bit" v-if="showBit">human-designed</span> AI treat people.</p>

<h3 id="and-treatment-of-humans-by-human-designed-ai">And Treatment of Humans by <span class="bit" v-if="showBit">human-designed</span> AI…</h3>

<p>More urgently, we need to consider the effects of present <span class="bit" v-if="showBit">human-designed</span> AI systems on human moral ideals.</p>

<p><span class="bit" v-if="showBit">human-designed</span> AI systems can promote human values. Low-cost automated medical diagnoses enable more accessible medical services. Fraud detection algorithms in banks help to prevent illegitimate transactions. Image recognition algorithms help to automatically detect images of child abuse and identify victims.</p>

<p>But <span class="bit" v-if="showBit">human-designed</span> AI can also violate human values. The use of generative models to create fake articles, videos and photos threatens our notion of truth. The use of facial recognition on public cameras disrupt our conventional understanding of privacy. The use of biased algorithms to hire workers and sentence criminals violate our values of fairness and justice.</p>

<p>The pervasive nature of <span class="bit" v-if="showBit">human-designed</span> AI systems means that these systems potentially affect millions and billions of lives. Many important institutions (political, judicial, financial) are increasingly augmented by <span class="bit" v-if="showBit">human-designed</span> AI systems. In short, it is critical to get things right before human civilization blows up in our faces. <span class="bit" v-if="showBit">human-designed</span> AI ethics goes beyond philosophical musings and thought experiments. It tries to fix the real problems cropping up from our new <span class="bit" v-if="showBit">human-designed</span> AI solutions.</p>

<h3 id="-which-are-also-designed-by-humans">… Which are also Designed by Humans</h3>

<p>For now at least, the implementation of <span class="bit" v-if="showBit">human-designed</span> AI systems is a manual non-automated process. So we really shouldn’t be thinking about how an <em>AI system</em> is violating human values. Keep in mind that the system was designed by humans and <em>its designers</em> are probably the ones who should be responsible for any ethical violations. In fact, all the instances of “AI” above should be replaced with “human-designed AI”, try clicking on this little red button to the right! <span class="tidbit-holder"><i class="plus circle large icon tidbit-link" :class="{ closed: showBit }" v-on:click="showBit = !showBit"></i></span></p>

<p>As such, <span class="bit" v-if="showBit">human-designed</span> AI ethics also consists of educating <span class="bit" v-if="showBit">human-designed</span> AI parents (aka human researchers and engineers) about how to bring up their <span class="bit" v-if="showBit">human-designed</span> AI babies. Because their <span class="bit" v-if="showBit">human-designed</span> AI babies grow up to become really influential <span class="bit" v-if="showBit">human-designed</span> AI adults. <span class="bit" v-if="showBit">human-designed</span> AI researchers and engineers <em>have</em> to understand the tremendous power and responsibility that they now possess.</p>

<div class="box-red">
<div class="ui list">
  <div class="item">
    <i class="check circle icon"></i>
    <div class="content">
        What do we mean by <span class="bit" v-if="showBit">human-designed</span> AI ethics?
    </div>
  </div>
</div>
<p class="emph">
  For the rest of this guide, <span class="bit" v-if="showBit">human-designed</span> AI ethics refers to the study of how <span class="bit" v-if="showBit">human-designed</span> AI systems promote and violate human values, including justice, autonomy and privacy. In particular, we note that current AI systems are still created, deployed and maintained by humans. And these humans need to start paying attention to how their systems are changing the world.
</p>
</div>

<hr />

<h2 id="artificial-intelligence-systems-ais"><span class="bit" v-if="showBit">Human-Designed</span> Artificial Intelligence Systems<br />(<span class="bit" v-if="showBit">Human-Designed</span> AIS)</h2>

<blockquote>
  <p>An [Artificial Intelligence System (AIS)] is any computing system using artificial intelligence algorithms, whether it’s software, a connected object or a robot.</p>
</blockquote>

<p><em>The Montréal Declaration, 2018 <dt-cite cite="montreal2018"></dt-cite></em></p>

<p>The Montréal Declaration is a set of <span class="bit" v-if="showBit">human-designed</span> AI ethics guidelines initiated by Université de Montréal. In the Declaration, its 10 principles refers extensively to “AIS” instead of “AI”. This guide will do the same because the term “system” serves as a nice reminder that we are looking at a complex network of parts that work together to make a prediction.</p>

<div>
<img class="comic" width="815px" src="/assets/comics/siri.png" title="Before working at Apple, Siri acted at Studio Ghibli." alt="Siri is not a tiny sprite living in an iPhone." />
</div>

<p class="box-blue">
  Siri is not a tiny sprite that lives in iPhones. Siri is an entire digital supply chain from initial conception to data collection to model training to deployment to maintenance and finally retirement. 
</p>

<p>The same is true for any other <span class="bit" v-if="showBit">human-designed</span> AIS, including Google Translate, Amazon Rekognition and Northpointe’s COMPAS. This big-picture perspective is important. It reminds us that we have to look at the entire system and infrastructure when we talk about <span class="bit" v-if="showBit">human-designed</span> AI ethics.</p>

<p>In addition to a digital supply chain, <span class="bit" v-if="showBit">human-designed</span> AIS also have physical supply chains that comprise energy usage, resource extraction and hardware recycling or disposal. These physical supply chains can be due to cloud servers, physical devices or simply the electricity and hardware used to train and house the models. The AI Now Institute also has a fantastic illustration titled <a href="https://anatomyof.ai/">Anatomy of an AI System</a> that considers <span class="bit" v-if="showBit">human-designed</span> AIS in terms of “material resources, human labor, and data”.</p>

<p>Finally, the “system” also includes the sociotechnical context where the <span class="bit" v-if="showBit">human-designed</span> AIS is applied. This refers to the culture, norms and values of the application, the domain and the geography and society that the application lives in. These values can be formalized (e.g. laws) or informal (e.g. unwritten customs and traditions). This sociotechnical context becomes critical when we talk about concepts like fairness and justice.</p>

<div class="box-red">
<div class="ui list">
  <div class="item">
    <i class="check circle icon"></i>
    <div class="content">
        What do we mean by <span class="bit" v-if="showBit">human-designed</span> AI systems?
    </div>
  </div>
</div>
<p class="emph">
  The term Artificial Intelligence System (AIS) refer to the entirety of <span class="bit" v-if="showBit">human-designed</span> artificial intelligence applications or solutions, in terms of:
</p>
<ul class="emph">
    <li>Digital lifecycle (conceptualization to retirement),</li>
    <li>Physical lifecycle (resource extraction to hardware disposal), and</li>
    <li>Sociotechnical context (culture, norms and values).</li>
  </ul>
</div>

<hr />

<h2 id="what-is-different-about-ai">What is different about <span class="bit" v-if="showBit">human-designed</span> AI?</h2>

<p>There’s been many articles talking about how <span class="bit" v-if="showBit">human-designed</span> AI is <em>the shit</em> and how it’s better than every other technology we’ve had. Here we look at three aspects that make <span class="bit" v-if="showBit">human-designed</span> AI stand out in terms of its social impact - an illusion of fairness, tremendous speed and scale, and open accessibility.</p>

<h3 id="illusion-of-fairness">Illusion of Fairness</h3>

<p>Since machines have no emotions, we often assume that they would be impartial and make decisions without fear or favor.</p>

<p>This assumption is flawed. For one, guns too, have no capacity for prejudice or bias. But we don’t attribute impartiality to guns. “Guns don’t kill people, people kill people.” A gun wielded by different people can have vastly different moral embeddings. The same can be said for <span class="bit" v-if="showBit">human-designed</span> AIS.</p>

<p>Moreover, the data used to train machine learning models can be a tremendous source of bias. A hiring model trained with sexist employment records would obviously suggest similarly sexist decisions. A recidivism model trained on racist arrest histories would obviously give racist suggestions. Like produces like. Garbage in, garbage out.</p>

<p>Unfortunately, <span class="bit" v-if="showBit">human-designed</span> AIS marketed as impartial and unbiased seem really appealing for all sorts of important decisions. This illusion of fairness provides unwarranted justification for widespread deployment of <span class="bit" v-if="showBit">human-designed</span> AIS without adequate control. But fairness is not inherent in <span class="bit" v-if="showBit">human-designed</span> AIS. It is a quality that has to be carefully designed for and maintained.</p>

<h3 id="speed-and-scale">Speed and Scale</h3>

<p>The shipping industry revolutionized trade, enabling it to be conducted on an international scale across maritime trade routes. Previously lengthy land detours had much quicker maritime alternatives. But this increase in speed and scale also facilitated the rapid spread of the Black Death.</p>

<p>Many of today’s <span class="bit" v-if="showBit">human-designed</span> AIS function on an unprecedented speed and scale. Google Translate serves over 500 million queries a day. Amazon’s Rekognition claims to be able to perform “real-time face recognition across tens of millions of faces”. Previously expensive, slow, one-to-one functions can now be automated to become cheaper, faster and serve much larger audiences. This means more people can benefit from <span class="bit" v-if="showBit">human-designed</span> AIS.</p>

<p>But just like the Black Death supercharged by rats on merchant ships, this crazy speed and scale also applies to any inherent problems. A biased translation system could serve over 500 million biased queries a day. An insecure facial recognition system can leak tens of millions of faces and related personal details. Speed and scale is a double-edged sword and it’s surprising how people often forget that a double-edged sword is double-edged.</p>

<div>
<img class="comic" width="500px" src="/assets/comics/scale.png" title="It's a Rock Fact!" alt="Speed and scale applies to both benefits and problems." />
</div>

<h3 id="accessibility">Accessibility</h3>

<p><span class="bit" v-if="showBit">Human-designed</span> AI research has largely been open. As a self-taught coder and <span class="bit" v-if="showBit">human-designed</span> AI researcher, I remain eternally grateful for the kindness and generosity of the <span class="bit" v-if="showBit">human-designed</span> AI community. The vast majority of researchers share their work freely on arxiv.org and GitHub. Open-source software libraries and datasets are available to anyone with Internet access. There are abundant tutorials for anyone keen to train their own image recognition or language model.</p>

<p>Furthermore, advances in hardware mean that consumer-grade computers are sufficient to run many state-of-the-art algorithms. More resource-intensive algorithms can always be trained on the cloud via services such as Amazon Web Services, Google Cloud and Microsoft Azure.</p>

<p>The combination of accessible research, hardware, software and data means that many people have the ability to train and deploy their own <span class="bit" v-if="showBit">human-designed</span> AIS for personal use. A powerful technology is now openly accessible to unregulated individuals who may use it for any purpose they deem fit. There has been cool examples of students using Tensorflow to predict wildfires (<a href="https://www.blog.google/technology/ai/fighting-fire-machine-learning-two-students-use-tensorflow-predict-wildfires/">link</a>) and tons of other nice stuff (<a href="https://www.wired.com/story/diy-tinkerers-artificial-intelligence-smart-tech/">link</a>).</p>

<p>But like speed and scale, this accessibility is also a double-edged sword. Consider the examples of DeepFakes and DeepNude. These open-source programs use Generative Adversarial Networks and variants of the pix2pix algorithm to generate realistic pornographic media of unwitting individuals. Accessible and powerful technology can also be used by irresponsible or malicious actors.</p>

<div>
<img class="comic" width="350px" src="/assets/comics/accessibility.png" title="The axes are probably on logarithmic scales." alt="Accessibility versus Capacity for Harm." />
</div>

<div class="box-red">
<div class="ui list">
  <div class="item">
    <i class="check circle icon"></i>
    <div class="content">
        How is <span class="bit" v-if="showBit">human-designed</span> AI different from other technologies? 
    </div>
  </div>
</div>
<div class="emph">
<p>
  <span class="bit" v-if="showBit">Human-designed</span> AI differs from most technologies in three aspects:
</p>
<ul>
  <li>We tend to think <span class="bit" v-if="showBit">human-designed</span> AI is like totally fair and better than people.</li>
  <li><span class="bit" v-if="showBit">Human-designed</span> AI can be crazy fast and deployed on a massive scale.</li>
  <li>Given how powerful it is, <span class="bit" v-if="showBit">human-designed</span> AI is also really accessible to everyone.</li>
</ul>
</div>
</div>

<hr />

<h2 id="the-most-important-question">The Most Important Question</h2>

<p><em>*Cue drumroll*</em></p>

<h3 id="when-is-human-designed-ai-not-the-answer">“When is <span class="bit" v-if="showBit">human-designed</span> AI not the answer?”</h3>

<div>
<img class="comic" width="250px" src="/assets/comics/notanswer_1.png" title="" alt="Is that a trick question?" />
</div>

<p>This is the most important question in this entire guide, and these days it can feel like the answer is, “Never.”</p>

<p class="emph box-red">
  This section here is to remind the reader that not using <span class="bit" v-if="showBit">human-designed</span> AIS <em>is</em> an option.
</p>

<p><span class="bit" v-if="showBit">Human-designed</span> AI technologies have been used for facial recognition, hiring, criminal sentencing, credit scoring. More unconventional applications include writing inspirational quotes (<a href="http://inspirobot.me/">link</a>), coming up with Halloween costumes (<a href="https://www.nytimes.com/interactive/2018/10/26/opinion/halloween-spooky-costumes-machine-learning-generator.html">link</a>), inventing new pizza recipes (<a href="https://www.youtube.com/watch?v=WVvHCJls3yY">link</a>) and creating rap lyrics (<a href="http://deepbeat.org/">link</a>).</p>

<div>
<img class="comic" width="300px" src="/assets/comics/notanswer_2.png" title="Linguine used Bind. It's super effective!" alt="Is that a trick question?" />
</div>

<p>But the superiority of <span class="bit" v-if="showBit">human-designed</span> AIS should not be taken for granted despite all the hype. For example, human professionals are often far better at explaining their decisions, as compared to <span class="bit" v-if="showBit">human-designed</span> AIS. Most humans also tend to make better jokes.</p>

<p>It is immensely important to consider the trade-offs when deploying <span class="bit" v-if="showBit">human-designed</span> AIS and look critically at both pros and cons. In some cases, <span class="bit" v-if="showBit">human-designed</span> AIS may not actually offer significant benefits despite all the hype. Common considerations include explainability and emotional and social qualities, where humans far outperform machines.</p>

<h3 id="human-designed-ai--human--best-of-both-worlds"><span class="bit" v-if="showBit">Human-designed</span> AI + Human = Best of Both Worlds?</h3>

<p><span class="bit" v-if="showBit">Human-designed</span> AI+Human systems are frequently perceived to be the best of both worlds. We have the empathy and explainability of humans augmented by the rigour and repeatability of <span class="bit" v-if="showBit">human-designed</span> AI systems. What could go wrong? Well, turns out documented experiences have shown that in such systems, humans might have a tendency to defer to suggestions made by the <span class="bit" v-if="showBit">human-designed</span> AIS. So rather than “<span class="bit" v-if="showBit">Human-Designed</span> AI+Human”, these systems are more like “<span class="bit" v-if="showBit">Human-Designed</span> AI+AgreeableHuman”.</p>

<div class="box-blue">

<p>
  In her book Automating Inequality, Virginia Eubanks notes that child welfare officers working with a child abuse prediction model would choose to amend their own assessments in light of the model's predictions.
</p>

<blockquote>
  <p>
    Though the screen that displays the [Allegheny Family Screening Tool (AFST)] score states clearly that the system "is not intended to make investigative or other child welfare decisions," an ethical review released in May 2016 by Tim Dare from the University of Auckland and Eileen Gambrill from University of California, Berkeley, cautions that the AFST risk score might be compelling enough to make intake workers question their own judgement.
  </p>
</blockquote>
<blockquote>
  <p>
    According to Vaithianathan and Putnam-Hornstein, <b>intake screeners have asked for the ability to go back and change their risk assessments after they see the AFST score</b>, suggesting that they believe that the model is less fallible than human screeners.
  </p>
</blockquote>

<p><em>Automating Inequality (Virginia Eubanks, 2018) <dt-cite cite="eubanks2018automating"></dt-cite></em></p>
</div>

<p>Such observations are hardly surprising, given the daily exhortations of the reliability of machines. In fact, the human tendency to defer to automated decisions has been termed “automation bias” <dt-cite cite="skitka2000automation,citron2007technological"></dt-cite>. Unfortunately, this over-deference to machines potentially undermines the mutually complementary aspect of <span class="bit" v-if="showBit">human-designed</span> AI+Human models.</p>

<div>
<img class="comic" width="300px" src="/assets/comics/notanswer_3.png" title="The teller said she would be happy to help me reinstate my account if I can prove I am alive." alt="Over-deference to machines." />
</div>

<h3 id="neglected-ripples">Neglected Ripples</h3>

<p>More generally, when discussing the pros and cons of adopting <span class="bit" v-if="showBit">human-designed</span> AIS solutions, we often forget to consider how the <span class="bit" v-if="showBit">human-designed</span>  AIS might affect the humans interacting with the system i.e. cause “ripples” within the system. This is referred to the Ripple Effect Trap by Selbst et al. <dt-cite cite="selbst2019fairness"></dt-cite>. Examples of ripples include:</p>

<ul>
  <li>Automation bias, as mentioned earlier. <tidbit content="This refers to an unwarranted bias towards automated decisions. This might occur when people lack confidence in their own decisions, such as new or untrained personnel. It might also occur when the decision has severe consequences. People afraid of taking the blame for a wrong decision might prefer to transfer responsibility to the &lt;span class=&quot;bit&quot; v-if=&quot;showBit&quot;&gt;human-designed&lt;/span&gt; AIS."></tidbit></li>
  <li>Automation aversion. <tidbit content="The opposite of automation bias, this refers to a preference to disagree with automated decisions. This can arise from a fear of being displaced - &quot;They took our jobs!&quot; It can also be due to a bad history with poorly designed &lt;span class=&quot;bit&quot; v-if=&quot;showBit&quot;&gt;human-designed&lt;/span&gt; AIS or general mistrust due to negative media portrayals."></tidbit></li>
  <li>Overconfidence in AIS-derived decisions. <tidbit content="While the well-known fallibility of humans remind us to double and triple check decisions, employing &lt;span class=&quot;bit&quot; v-if=&quot;showBit&quot;&gt;human-designed&lt;/span&gt; AIS might create a false sense of security. This can arise over long-term experience with a generally reliable &lt;span class=&quot;bit&quot; v-if=&quot;showBit&quot;&gt;human-designed&lt;/span&gt; AIS. People might gradually take for granted the reliability of the &lt;span class=&quot;bit&quot; v-if=&quot;showBit&quot;&gt;human-designed&lt;/span&gt; AIS. Consider the excruciating experiences of test drivers for self-driving cars, who have to be continuously alert despite a mostly safe ride."></tidbit></li>
</ul>

<div class="box-red">
<div class="ui list">
  <div class="item">
    <i class="check circle icon"></i>
    <div class="content">
        What is the single most important question when implementing <span class="bit" v-if="showBit">human-designed</span> AI solutions?  
    </div>
  </div>
</div>
<div class="emph">
<p>
  "Is using <span class="bit" v-if="showBit">human-designed</span> AI for this <em>really</em> a good idea?"
</p>
<p>
  In other words, think hard about what using <span class="bit" v-if="showBit">human-designed</span> AI really means in the context of your problem. Like really hard. Not using <span class="bit" v-if="showBit">human-designed</span> AI is definitely an option.
</p>
<p>
  And don't assume that <span class="bit" v-if="showBit">human-designed</span> AI+Human systems are definitely better than <span class="bit" v-if="showBit">human-designed</span> AI or humans by themselves. Instead, consider how <span class="bit" v-if="showBit">human-designed</span> AI and people might interact within your problem in unexpected ways. Ask prospective users what they think about <span class="bit" v-if="showBit">human-designed</span> AIS and factor their responses into your mental models.
</p>
</div>
</div>

<tofro prevtext="Intro" prevlink="../" nexttext="What's Up with Fairness?" nextlink="../fairness/"></tofro>

<h2 id="references">References</h2>

<dt-bibliography></dt-bibliography>

<script type="text/bibliography">

@inproceedings{montreal2018,
  title={The Montréal Declaration},
  author={Université de Montréal},
  booktitle={The Montréal Declaration for a Responsible Development of Artificial Intelligence},
  pages={1-308},
  year={2018},
  organization={Université de Montréal},
  url={https://www.montrealdeclaration-responsibleai.com/}
}

@article{moor1985computer,
  title={What is computer ethics?},
  author={Moor, James H},
  journal={Metaphilosophy},
  volume={16},
  number={4},
  pages={266-275},
  year={1985},
  publisher={Wiley Online Library},
  url={https://doi.org/10.1111/j.1467-9973.1985.tb00173.x}
}

@book{tegmark2017life,
  title={Life 3.0: Being human in the age of artificial intelligence},
  author={Tegmark, Max},
  year={2017},
  publisher={Knopf}
}

@book{eubanks2018automating,
  title={Automating inequality: How high-tech tools profile, police, and punish the poor},
  author={Eubanks, Virginia},
  year={2018},
  publisher={St. Martin's Press}
}

@article{citron2007technological,
  title={Technological due process},
  author={Citron, Danielle Keats},
  journal={Wash. UL Rev.},
  volume={85},
  pages={1249},
  year={2007},
  publisher={HeinOnline},
  url={https://openscholarship.wustl.edu/cgi/viewcontent.cgi?article=1166&context=law_lawreview}
}

@article{skitka2000automation,
  title={Automation bias and errors: are crews better than individuals?},
  author={Skitka, Linda J and Mosier, Kathleen L and Burdick, Mark and Rosenblatt, Bonnie},
  journal={The International journal of aviation psychology},
  volume={10},
  number={1},
  pages={85-97},
  year={2000},
  publisher={Taylor & Francis},
  url={https://doi.org/10.1207/S15327108IJAP1001_5}
}

@inproceedings{selbst2019fairness,
  title={Fairness and abstraction in sociotechnical systems},
  author={Selbst, Andrew D and Boyd, Danah and Friedler, Sorelle A and Venkatasubramanian, Suresh and Vertesi, Janet},
  booktitle={Proceedings of the Conference on Fairness, Accountability, and Transparency},
  pages={59-68},
  year={2019},
  organization={ACM},
  url={http://sorelle.friedler.net/papers/sts_fat2019.pdf}
}
</script>

			</div>
		</div>
		<div id="citation-hover">
		</div><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <!-- <h2 class="footer-heading"></h2> -->

    <div class="footer-col-wrapper">

      <!-- <div class="footer-col footer-col-2"> --><ul class="social-media-list"><li><a href="https://github.com/greentfrapp"><svg class="svg-icon" viewBox="0 0 16 16"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <!-- <span class="username">greentfrapp</span> --></a></li><li><a href="https://www.linkedin.com/in/sweekiat"><svg class="svg-icon" viewBox="0 0 16 16"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <!-- <span class="username">sweekiat</span> --></a></li></ul>
<!-- </div> -->

      <!-- <div class="footer-col footer-col-3">
        <p></p>
      </div> -->
    </div>

  </div>

</footer>
<div id="cookieconsent"></div>
		</div><script src="/assets/js/js.cookie.js"></script>
<script src="/assets/js/bibtexParse.js"></script>
<!-- <script src="/assets/guide/bibtex.js"></script> -->
<script src="/assets/js/bibliography.js"></script>
<script src="/assets/js/textures.js"></script>
<script src="/assets/js/d3-annotation.js"></script>
<script src="/assets/js/guide_fairnessexplorable.js"></script>
<script src="/assets/js/guide_tidbit.js"></script>
<script src="/assets/js/guide_tofro.js"></script>
<script src="/assets/js/guide_main.js"></script>
<script src="https://cdn.jsdelivr.net/npm/cookieconsent@3/build/cookieconsent.min.js" data-cfasync="false"></script>
<script src="/assets/js/cookieconsent.js"></script></body>

</html>
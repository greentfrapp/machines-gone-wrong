<!DOCTYPE html>
<html lang="en"><head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta property="og:description" content="Inspired by recent ethical concerns in AI, Machines Gone Wrong is an online guide to help AI practitioners get up to date with socio-technical discussions surrounding AI systems." />
  	<meta property="og:image" content="https://greentfrapp.github.io/project-asimov/assets/frontispiece.jpg" />
	<link rel="shortcut icon" href="https://greentfrapp.github.io/assets/favicon.ico" type="image/x-icon">
	<link rel="icon" href="https://greentfrapp.github.io/assets/favicon.ico" type="image/x-icon"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Summary Checklist | Machines Gone Wrong</title>
<meta name="generator" content="Jekyll v3.7.4" />
<meta property="og:title" content="Summary Checklist | Machines Gone Wrong" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/checklist/" />
<meta property="og:url" content="http://localhost:4000/checklist/" />
<script type="application/ld+json">
{"@type":"WebPage","url":"http://localhost:4000/checklist/","headline":"Summary Checklist | Machines Gone Wrong","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/components/icon.min.css" />
	<link rel="stylesheet" href="/assets/semantic.min.css">
	<link rel="stylesheet" href="/assets/main.css">
	<script src="https://cdnjs.cloudflare.com/ajax/libs/d3/5.9.2/d3.min.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>
	<script src="https://unpkg.com/vue"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/showdown/1.9.0/showdown.min.js"></script>
	<script src="https://unpkg.com/intersection-observer@0.5.1/intersection-observer.js"></script>
	<!-- <script src="https://unpkg.com/scrollama"></script> -->
	<!-- <script src="/assets/guide/scroller.js"></script> -->
	<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/cookieconsent@3/build/cookieconsent.min.css" />
	<script type="text/front-matter">
	title: "Machines Gone Wrong"
	description: "Inspired by recent ethical concerns in AI, Machines Gone Wrong is an online guide to help AI practitioners get up to date with socio-technical discussions surrounding AI systems."
	authors:
	- Lim Swee Kiat: https://greentfrapp.github.io
	affiliations:
	- Singapore University of Technology and Design: https://urbanscience.sutd.edu.sg/
	</script>
</head><body id="guide-body" ><div class="ui sidebar inverted vertical menu light">
  <a class="item chapter" href="/">
    Introduction
  </a>
  <a class="item chapter" href="/basics/">
    Getting Started
  </a>
  <a class="item subsection" href="/basics/#ethics-of-artificial-intelligence-ai-ethics">
    Ethics of Artificial Intelligence
  </a>
  <a class="item subsection" href="/basics/#artificial-intelligence-systems-ais">
    Artificial Intelligence Systems
  </a>
  <a class="item subsection" href="/basics/#what-is-different-about-ai">
    What is different about AI?
  </a>
  <a class="item subsection" href="/basics/#the-most-important-question">
    The Most Important Question
  </a>
  <a class="item chapter" href="/fairness/">
    Algorithmic Bias
  </a>
  <a class="item section" href="/fairness/">
    Understanding Fairness
  </a>
  <a class="item subsection" href="/fairness/#disparate-treatment-disparate-impact">
    Disparate Treatment and Disparate Impact
  </a>
  <a class="item subsection" href="/fairness/#a-fair-fat-pet-predictor">
    A Fair Fat Pet Predictor
  </a>
	<a class="item subsection" href="/fairness/#the-impossibility-theorem">
    The Impossibility Theorem
  </a>
  <a class="item subsection" href="/fairness/#context-free-fairness">
    Context-Free Fairness
  </a>
  <a class="item subsection" href="/fairness/#learning-about-the-context">
    Learning about the Context
  </a>
  <a class="item section" href="/bias_i/">
    Understanding Bias I
  </a>
  <a class="item subsection" href="/bias_i/#two-types-of-harms">
    Two Types of Harms
  </a>
  <a class="item subsection" href="/bias_i/#harms-of-allocation">
    Harms of Allocation
  </a>
  <a class="item subsection" href="/bias_i/#harms-of-representation">
    Harms of Representation
  </a>
  <a class="item section" href="/bias_ii/">
    Understanding Bias II
  </a>
  <a class="item subsection" href="/bias_ii/#bias-from-data">
    Bias from Data
  </a>
  <a class="item subsection" href="/bias_ii/#bias-from-algorithm-design">
    Bias from Algorithm Design
  </a>
  <a class="item subsection" href="/bias_ii/#bias-from-deployment">
    Bias from Deployment
  </a>
  <a class="item section" href="/checklist/">
    Summary Checklist
  </a>
  <a class="item section" href="/resources/">
    Resources
  </a>
  <!-- <a class="item subsection" href="/resources/#tools">
    Tools
  </a>
  <a class="item subsection" href="/resources/#datasets">
    Datasets
  </a>
  <a class="item subsection" href="/resources/#readings">
    Readings
  </a> -->
  <a class="item chapter" href="/about/">
    About
  </a><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <!-- <h2 class="footer-heading"></h2> -->

    <div class="footer-col-wrapper">

      <!-- <div class="footer-col footer-col-2"> --><ul class="social-media-list"><li><a href="https://github.com/greentfrapp"><svg class="svg-icon" viewBox="0 0 16 16"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <!-- <span class="username">greentfrapp</span> --></a></li><li><a href="https://www.linkedin.com/in/sweekiat"><svg class="svg-icon" viewBox="0 0 16 16"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <!-- <span class="username">sweekiat</span> --></a></li></ul>
<!-- </div> -->

      <!-- <div class="footer-col footer-col-3">
        <p></p>
      </div> -->
    </div>

  </div>

</footer>
</div><div id="app" class="pusher light">
		<div id="nav" class="fixed" :class="{light: !darkmode}">
			<div class="logo">
				<i class="icon bars large toggle-menu"></i>
				<div style="width: 140px; display: inline-block; vertical-align: middle;" class="toggle-menu">
	  			<a> 
					Machines Gone Wrong
				</a>
				</div>
				<div class="ui toggle checkbox" id="dark-mode-switch">
				  <input type="checkbox" name="darkmode" v-model="darkmode" @click="toggleDarkmode">
				  <label>Dark Mode</label>
				</div>
			</div>
		</div>
		<div :class="{light: !darkmode}">
			<div class="content guide-content">
				<h1 id="summary-checklist">Summary Checklist</h1>

<hr />

<p>Here is a checklist of questions and prompts to ask when implementing an AIS. While there is no strictly correct answer, a good rule of thumb is that we should be okay with publishing our answers publicly.</p>

<p>Again, this checklist is best completed as a group exercise and with extensive inputs from users and people who might interact with the proposed AIS.</p>

<hr />

<h3 id="section-1---understanding-the-context">Section 1 - Understanding the Context</h3>

<h4 id="general-context">General Context</h4>

<ol>
  <li>What is the ultimate aim of the application? <tidbit content="&lt;br/&gt;For example, for recidivism prediction, the true objective might be to make the society a safer place. As part of that, we want to identify individuals who might be prone to reoffending and offer them additional help to reduce future crime. Note the many implicit assumpations here. We assume that our sub-goal contributes to our objective. We also assume that reoffending is something that can be reliably predicted."></tidbit></li>
  <li>What are the pros and cons of an AIS versus other solutions? <tidbit content="&lt;br/&gt;The main point here is to first weigh all the possible solutions instead of just implementing an AIS immediately."></tidbit></li>
  <li>How is the AIS supposed to be used? <tidbit content="&lt;br/&gt;By answering this question, we can begin to think of ways that we can 'nudge' users towards the desired usage, as well as ways that the AIS can be misused."></tidbit></li>
  <li>What is the current system that the AIS will be replacing? <tidbit content="&lt;br/&gt;How is the problem being solved at the moment? How is the proposed AIS better than this solution? How is it worse?"></tidbit></li>
  <li>Who will interact with the AIS? <tidbit content="&lt;br/&gt;This probably includes more than just the direct users that benefit from the AIS. Hiring models, for instance, interact with both employers (direct users) and job applicants."></tidbit></li>
  <li>Create a few user personas - the technophobe, the newbie etc. - and think about how they might react to the AIS across the short-term and long-term. <tidbit content="&lt;br/&gt;This question examines the 'ripples' that the AIS might cause when it is implemented, ranging from the short-term to the long-term."></tidbit></li>
  <li>Think of ways that the AIS can be misused by unknowning or malicious actors. <tidbit content="&lt;br/&gt;How can we design the AIS to prevent these misuses? If the potential harm is too great, we might want to reconsider adopting an AIS solution."></tidbit></li>
</ol>

<h4 id="about-fairness">About Fairness</h4>

<ol>
  <li>What do false positives and false negatives mean for different users? Under what circumstances might one be worse than the other? <tidbit content="&lt;br/&gt;In recidivism prediction models for instance, false positives mean innocent people were wrongly accused. When we step from theory to the real world, we need to see that these mathematical concepts have very real meanings."></tidbit></li>
  <li>Try listing out some examples of fair and unfair predictions. Why are they fair/unfair? <tidbit content="&lt;br/&gt;This is the first step towards trying to understand what are the protected traits in this context and how we should define fairness."></tidbit></li>
  <li>What are the relevant protected traits in this problem? <tidbit content="&lt;br/&gt;Common protected traits include gender, skin color, ethnicity, age and physical ability. But remember that this really depends on the context and the culture that the application is situated in."></tidbit></li>
  <li>Which fairness metrics should we prioritize? <tidbit content="&lt;br/&gt;Prioritizing means that some metrics are invariably compromised or violated. These decisions and their resultant shortcomings should be made known to users."></tidbit></li>
</ol>

<hr />

<h3 id="section-2---preparing-the-data">Section 2 - Preparing the Data</h3>

<ol>
  <li>What is our population? <tidbit content="&lt;br/&gt;Note that this refers to the population that comprises all the possible inputs to the proposed AIS. This is important because it affects how we collect our data and evaluate our models later on. See &lt;a href='../bias_ii#defining-the-population'&gt;Understanding Bias II&lt;/a&gt; for details."></tidbit></li>
  <li>How does our dataset distribution differ from our population distribution? <tidbit content="&lt;br/&gt;In most cases, the dataset collected is different from the population. This is okay, but we have to be clear about how it is different and be aware of possible problems that might arise from the mismatch. See &lt;a href='../bias_ii#training-dataset-versus-population'&gt;Understanding Bias II&lt;/a&gt; for details."></tidbit></li>
  <li>Are we measuring the features/labels the same way for different groups? <tidbit content="&lt;br/&gt;Bias can creep in when we collect data differently for different groups. Check out &lt;a href='../bias_i#proxy-labels'&gt;Understanding Bias I&lt;/a&gt; for an example."></tidbit></li>
  <li>How are our annotated labels different from the ideal labels? <tidbit content="&lt;br/&gt;Often, the labels that we really want is impossible or prohibitively expensive to obtain and we settle for proxy labels. Here, we ask, 'Are we using proxy labels?' and 'What are possible problems from using proxy labels?' See &lt;a href='../bias_ii#the-target-variable'&gt;Understanding Bias II&lt;/a&gt; for details."></tidbit></li>
</ol>

<hr />

<h3 id="section-3---training-the-model">Section 3 - Training the Model</h3>

<ol>
  <li>How do our input features relate to our protected traits? <tidbit content="&lt;br/&gt;In cases where input features &lt;em&gt;are&lt;/em&gt; protected traits, we need to justify their use in the model or remove them. We also need to check for correlations between protected traits and our input features, to identify proxies for the protected traits. These proxies can also be a source of algorithmic bias. See &lt;a href='../bias_ii#input-features'&gt;Understanding Bias II&lt;/a&gt; for details."></tidbit></li>
  <li>Do we use the same model or different models for different inputs? <tidbit content="&lt;br/&gt;Using the same model assumes that the mapping between input samples and output prediction is the same for all groups, which might not be the case. On the other hand, training different models requires sufficient data for each model. See &lt;a href='../bias_ii#aggregation'&gt;Understanding Bias II&lt;/a&gt; for details."></tidbit></li>
  <li>If we are importing a pre-trained model or external data, what are possible conflicts between these imports and our current context? <tidbit content="&lt;br/&gt;Using pre-trained models and external datasets is a common practice. But these imported models and data can potentially carry hidden biases. See &lt;a href='../bias_ii#transfering-models-and-datasets'&gt;Understanding Bias II&lt;/a&gt; for details."></tidbit></li>
</ol>

<hr />

<h3 id="section-4---evaluating-the-model">Section 4 - Evaluating the Model</h3>

<ol>
  <li>How does our test distribution differ from our population distribution? <tidbit content="&lt;br/&gt;Similar to Section 2 above, we need to think about the differences between our test dataset and our real population and possible problems that might occur."></tidbit></li>
  <li>What can we <em>say</em> about the fairness of our final model? <tidbit content="&lt;br/&gt;More than just accuracy and other performance metrics, results from fairness metric evaluations should also be documented and made available to users. See &lt;a href='../bias_ii#evaluation'&gt;Understanding Bias II&lt;/a&gt; for details."></tidbit></li>
  <li>When we detect some unfairness with our metrics - is the disparity justified? <tidbit content="&lt;br/&gt;This lends some consideration for context to the quantification of fairness. Ultimately, how unjust a disparity is depends on the extent of disparity relative to its justification."></tidbit></li>
</ol>

<hr />

<h3 id="section-5---deploying-the-solution">Section 5 - Deploying the Solution</h3>

<ol>
  <li>How do we detect errors from the AIS after deployment? <tidbit content="&lt;br/&gt;The job's not over when the model is deployed. After emerging from the laboratory, the model needs to be continuously evaluated based on real-world data, to identify unexpected problems or model failure. Importantly, the model should not be caught in a self-enforcing feedback loop. See &lt;a href='../bias_ii#feedback'&gt;Understanding Bias II&lt;/a&gt; for details."></tidbit></li>
  <li>What are alternative solutions in case of failure? <tidbit content="&lt;br/&gt;Just like any other technology, the AIS can and will break down. How can we design for graceful degradation for all types of failures (e.g. wrong predictions, total failure)?"></tidbit></li>
  <li>How can we allow users to gracefully opt out of the AIS? <tidbit content="&lt;br/&gt;Presently, there are people who are uncomfortable with certain AIS due to privacy and other concerns. How can we design for 'graceful degradation' that allows these users to opt out with minimal hassle? See &lt;a href='../bias_ii#graceful-degradation'&gt;Understanding Bias II&lt;/a&gt; for details."></tidbit></li>
</ol>

<tofro prevtext="Sources of Bias" prevlink="../bias_ii/" nexttext="Resources" nextlink="../resources/"></tofro>

			</div>
		</div>
		<div id="citation-hover">
		</div><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <!-- <h2 class="footer-heading"></h2> -->

    <div class="footer-col-wrapper">

      <!-- <div class="footer-col footer-col-2"> --><ul class="social-media-list"><li><a href="https://github.com/greentfrapp"><svg class="svg-icon" viewBox="0 0 16 16"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <!-- <span class="username">greentfrapp</span> --></a></li><li><a href="https://www.linkedin.com/in/sweekiat"><svg class="svg-icon" viewBox="0 0 16 16"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <!-- <span class="username">sweekiat</span> --></a></li></ul>
<!-- </div> -->

      <!-- <div class="footer-col footer-col-3">
        <p></p>
      </div> -->
    </div>

  </div>

</footer>
<div id="cookieconsent"></div>
		</div><script src="/assets/js/js.cookie.js"></script>
<script src="/assets/js/bibtexParse.js"></script>
<!-- <script src="/assets/guide/bibtex.js"></script> -->
<script src="/assets/js/bibliography.js"></script>
<script src="/assets/js/textures.js"></script>
<script src="/assets/js/d3-annotation.js"></script>
<script src="/assets/js/guide_fairnessexplorable.js"></script>
<script src="/assets/js/guide_tidbit.js"></script>
<script src="/assets/js/guide_tofro.js"></script>
<script src="/assets/js/guide_main.js"></script>
<script src="https://cdn.jsdelivr.net/npm/cookieconsent@3/build/cookieconsent.min.js" data-cfasync="false"></script>
<script src="/assets/js/cookieconsent.js"></script></body>

</html>
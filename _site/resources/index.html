<!DOCTYPE html>
<html lang="en"><head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta property="og:description" content="Inspired by recent ethical concerns in AI, Machines Gone Wrong is an online guide to help AI practitioners get up to date with socio-technical discussions surrounding AI systems." />
  	<meta property="og:image" content="https://greentfrapp.github.io/project-asimov/assets/frontispiece.jpg" />
	<link rel="shortcut icon" href="https://greentfrapp.github.io/assets/favicon.ico" type="image/x-icon">
	<link rel="icon" href="https://greentfrapp.github.io/assets/favicon.ico" type="image/x-icon"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Resources | Machines Gone Wrong</title>
<meta name="generator" content="Jekyll v3.7.4" />
<meta property="og:title" content="Resources | Machines Gone Wrong" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/resources/" />
<meta property="og:url" content="http://localhost:4000/resources/" />
<script type="application/ld+json">
{"@type":"WebPage","url":"http://localhost:4000/resources/","headline":"Resources | Machines Gone Wrong","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/components/icon.min.css" />
	<link rel="stylesheet" href="/assets/semantic.min.css">
	<link rel="stylesheet" href="/assets/main.css">
	<script src="https://cdnjs.cloudflare.com/ajax/libs/d3/5.9.2/d3.min.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/semantic-ui/2.4.1/semantic.min.js"></script>
	<script src="https://unpkg.com/vue"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/showdown/1.9.0/showdown.min.js"></script>
	<script src="https://unpkg.com/intersection-observer@0.5.1/intersection-observer.js"></script>
	<!-- <script src="https://unpkg.com/scrollama"></script> -->
	<!-- <script src="/assets/guide/scroller.js"></script> -->
	<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/cookieconsent@3/build/cookieconsent.min.css" />
	<script type="text/front-matter">
	title: "Machines Gone Wrong"
	description: "Inspired by recent ethical concerns in AI, Machines Gone Wrong is an online guide to help AI practitioners get up to date with socio-technical discussions surrounding AI systems."
	authors:
	- Lim Swee Kiat: https://greentfrapp.github.io
	affiliations:
	- Singapore University of Technology and Design: https://urbanscience.sutd.edu.sg/
	</script>
</head><body id="guide-body" ><div class="ui sidebar inverted vertical menu light">
  <a class="item chapter" href="/">
    Introduction
  </a>
  <a class="item chapter" href="/basics/">
    Getting Started
  </a>
  <a class="item subsection" href="/basics/#ethics-of-artificial-intelligence-ai-ethics">
    Ethics of Artificial Intelligence
  </a>
  <a class="item subsection" href="/basics/#artificial-intelligence-systems-ais">
    Artificial Intelligence Systems
  </a>
  <a class="item subsection" href="/basics/#what-is-different-about-ai">
    What is different about AI?
  </a>
  <a class="item subsection" href="/basics/#the-most-important-question">
    The Most Important Question
  </a>
  <a class="item chapter" href="/fairness/">
    Algorithmic Bias
  </a>
  <a class="item section" href="/fairness/">
    Understanding Fairness
  </a>
  <a class="item subsection" href="/fairness/#disparate-treatment-disparate-impact">
    Disparate Treatment and Disparate Impact
  </a>
  <a class="item subsection" href="/fairness/#a-fair-fat-pet-predictor">
    A Fair Fat Pet Predictor
  </a>
	<a class="item subsection" href="/fairness/#the-impossibility-theorem">
    The Impossibility Theorem
  </a>
  <a class="item subsection" href="/fairness/#context-free-fairness">
    Context-Free Fairness
  </a>
  <a class="item subsection" href="/fairness/#learning-about-the-context">
    Learning about the Context
  </a>
  <a class="item section" href="/bias_i/">
    Understanding Bias I
  </a>
  <a class="item subsection" href="/bias_i/#two-types-of-harms">
    Two Types of Harms
  </a>
  <a class="item subsection" href="/bias_i/#harms-of-allocation">
    Harms of Allocation
  </a>
  <a class="item subsection" href="/bias_i/#harms-of-representation">
    Harms of Representation
  </a>
  <a class="item section" href="/bias_ii/">
    Understanding Bias II
  </a>
  <a class="item subsection" href="/bias_ii/#bias-from-data">
    Bias from Data
  </a>
  <a class="item subsection" href="/bias_ii/#bias-from-algorithm-design">
    Bias from Algorithm Design
  </a>
  <a class="item subsection" href="/bias_ii/#bias-from-deployment">
    Bias from Deployment
  </a>
  <a class="item section" href="/checklist/">
    Summary Checklist
  </a>
  <a class="item section" href="/resources/">
    Resources
  </a>
  <!-- <a class="item subsection" href="/resources/#tools">
    Tools
  </a>
  <a class="item subsection" href="/resources/#datasets">
    Datasets
  </a>
  <a class="item subsection" href="/resources/#readings">
    Readings
  </a> -->
  <a class="item chapter" href="/about/">
    About
  </a><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <!-- <h2 class="footer-heading"></h2> -->

    <div class="footer-col-wrapper">

      <!-- <div class="footer-col footer-col-2"> --><ul class="social-media-list"><li><a href="https://github.com/greentfrapp"><svg class="svg-icon" viewBox="0 0 16 16"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <!-- <span class="username">greentfrapp</span> --></a></li><li><a href="https://www.linkedin.com/in/sweekiat"><svg class="svg-icon" viewBox="0 0 16 16"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <!-- <span class="username">sweekiat</span> --></a></li></ul>
<!-- </div> -->

      <!-- <div class="footer-col footer-col-3">
        <p></p>
      </div> -->
    </div>

  </div>

</footer>
</div><div id="app" class="pusher light">
		<div id="nav" class="fixed" :class="{light: !darkmode}">
			<div class="logo">
				<i class="icon bars large toggle-menu"></i>
				<div style="width: 140px; display: inline-block; vertical-align: middle;" class="toggle-menu">
	  			<a> 
					Machines Gone Wrong
				</a>
				</div>
				<div class="ui toggle checkbox" id="dark-mode-switch">
				  <input type="checkbox" name="darkmode" v-model="darkmode" @click="toggleDarkmode">
				  <label>Dark Mode</label>
				</div>
			</div>
		</div>
		<div :class="{light: !darkmode}">
			<div class="content guide-content">
				<h1 id="resources">Resources</h1>

<hr />

<h3 id="offline-guide">Offline Guide</h3>

<p>An offline copy of this guide. Interactive components, such as the <a href="../fairness/#a-fair-fat-pet-predictor">explorable</a>, are left out. But it has been nicely formatted for you to print on sheets of dead tree matter.</p>

<p><a href="./MachinesGoneWrong.pdf">Download the offline guide</a></p>

<h3 id="other-websites-and-guides">Other Websites and Guides</h3>

<p>Useful or interesting links related to algorithmic bias.</p>

<ul>
  <li><strong><a href="https://www.survivalofthebestfit.com/">Survival of the Best Fit</a></strong> - a game about algorithmic bias in hiring</li>
  <li>Google’s <strong><a href="https://pair.withgoogle.com/">People + AI Guidebook</a></strong> and <strong><a href="https://cloud.google.com/inclusive-ml/">Inclusive ML Guide</a></strong></li>
  <li>The <strong><a href="http://www.uio.no/studier/emner/sv/oekonomi/ECON4135/h09/undervisningsmateriale/FinancialModelersManifesto.pdf">Financial Modelers’ Manifesto</a></strong> written by Emanuel Derman and Paul Wilmott was written for quants and financial engineers amidst the fallout of the subprime mortgage crisis, but the lessons are very applicable to today’s AI engineers <tidbit content="&lt;blockquote&gt;&lt;h4 style='margin-top:10px;'&gt;The Modelers' Hippocratic Oath&lt;/h4&gt;&lt;ul&gt;&lt;li&gt;I will remember that I didn't make the world, and it doesn't satisfy my equations.&lt;/li&gt;&lt;li&gt;Though I will use models boldly to estimate value, I will not be overly impressed by mathematics.&lt;/li&gt;&lt;li&gt;I will never sacrifice reality for elegance without explaining why I have done so.&lt;/li&gt;&lt;li&gt;Nor will I give the people who use my model false comfort about its accuracy. Instead, I will make explicit its assumptions and oversights.&lt;/li&gt;&lt;li&gt;I understand that my work may have enormous effects on society and the economy, many of them beyond my comprehension.&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;"></tidbit></li>
</ul>

<!-- - Andreesen Horowitz's [AI Playbook](http://aiplaybook.a16z.com/) -->

<h3 id="organizations-and-conferences">Organizations and Conferences</h3>

<ul>
  <li>The <strong><a href="https://ainowinstitute.org/">AI Now Institute</a></strong> is working actively on AI ethics and has many great <a href="https://ainowinstitute.org/reports.html">publications</a></li>
  <li><strong><a href="https://www.fatml.org/">FAT ML</a></strong> and <strong><a href="https://fatconference.org/">ACM FAT*</a></strong> are two of the main conferences in AI ethics - check out the conference websites for related publications</li>
</ul>

<h3 id="datasets">Datasets</h3>

<p>Datasets for the more bias-aware.</p>

<ul>
  <li><strong>Gapminder’s <a href="https://www.gapminder.org/dollar-street/matrix">Dollar Street images</a></strong>, which was used by DeVries et al. <dt-cite cite="devries2019does"></dt-cite> in <em>Does Object Recognition Work for Everyone?</em> and comprises over 16,000 images from 60 different countries across 138 categories - a downloadable set can be found via <a href="https://github.com/greentfrapp/dollar-street-images/">my GitHub repository</a></li>
  <li><strong>Google’s <a href="https://ai.google/tools/datasets/open-images-extended-crowdsourced/">Open Images Extended - Crowdsourced</a></strong>,  - Google has also provided some notes on possible biases in this dataset <tidbit content="- retrieved from the &lt;a href='https://www.kaggle.com/c/inclusive-images-challenge/overview/inclusive-images-faq#stage1-biases'&gt;Kaggle FAQ&lt;/a&gt;: &lt;br/&gt;&lt;blockquote&gt;&lt;p&gt;While we have targeted specific geographical locations in the collection of the Challenge Stage 1 dataset, it does have some particular areas of over and under representation that we found in preliminary analysis and wish to describe briefly here. These include:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Images of people tend to under-represent people who appear to be elderly.&lt;/li&gt;&lt;li&gt;Images tagged Child tend to be seen mostly in the context of play.&lt;/li&gt;&lt;li&gt;Some Person-related categories, including Bartender, Police Officer, and several sports related tags, appear to be predominantly (but by no means entirely) male.&lt;/li&gt;&lt;li&gt;Some Person-related categories, including Teacher, appear to be predominantly (but by no means entirely) female.&lt;/li&gt;&lt;li&gt;Some Person-related categories, including Teacher, appear to be predominantly (but by no means entirely) female.&lt;/li&gt;&lt;li&gt;Images with people seem to be taken predominantly in urban rather than rural areas.&lt;/li&gt;&lt;li&gt;Images of people in traditional locale-specific dress such as Sari’s in India are relatively under-represented in this Challenge Stage 1 data set.&lt;/li&gt;&lt;li&gt;In images tagged Wedding, there does not appear to be representation of same-sex marriages.&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;"></tidbit></li>
  <li>Joy Buolamwini’s <strong>Gender Shades dataset</strong> <dt-cite cite="buolamwini2018gender"></dt-cite> can be requested <a href="https://www.ajlunited.org/gender-shades">here</a></li>
</ul>

<h3 id="tools">Tools</h3>

<p>Tools for diagnosing and mitigating algorithmic bias, complete with detailed tutorials.</p>

<ul>
  <li>IBM’s <strong><a href="http://aif360.mybluemix.net/">AI Fairness 360 Open Source Toolkit</a></strong></li>
  <li>Microsoft’s <strong><a href="https://github.com/microsoft/interpret">InterpretML</a></strong></li>
  <li>Tensorboard’s <strong><a href="https://pair-code.github.io/what-if-tool/index.html">What If</a></strong></li>
</ul>

<h3 id="readings">Readings</h3>

<p>Academic publications related to algorithmic bias that I found useful.</p>

<ul>
  <li><strong>Do Artifacts have Politics?</strong> (Winner, 1980) <dt-cite cite="winner1980artifacts"></dt-cite></li>
  <li><strong>Bias in Computer Systems</strong> (Friedman and Nissenbaum, 1996) <dt-cite cite="friedman1996bias"></dt-cite></li>
  <li><strong>Technologies of Humility</strong> (Jasanoff, 2007) <dt-cite cite="jasanoff2007technologies"></dt-cite></li>
  <li><strong>Big Data’s Disparate Impact</strong> (Barocas and Selbst, 2016) <dt-cite cite="barocas2016big"></dt-cite></li>
  <li><strong>Inherent Trade-offs in the Fair Determination of Risk Scores</strong> (Kleinberg et al., 2016) <dt-cite cite="kleinberg2016inherent"></dt-cite></li>
  <li><strong>Interventions over Predictions: Reframing the Ethical Debate for Actuarial Risk Assessment</strong> (Barabas et al., 2017) <dt-cite cite="barabas2017interventions"></dt-cite></li>
  <li><strong>Fairness Definitions Explained</strong> (Verma et al., 2018) <dt-cite cite="verma2018fairness"></dt-cite></li>
  <li><strong>Fairness and Abstraction in Sociotechnical Systems</strong> (Selbst et al., 2019) <dt-cite cite="selbst2019fairness"></dt-cite></li>
  <li><strong>A Framework for Understanding Unintended Consequences of Machine Learning</strong> (Suresh and Guttag, 2019) <dt-cite cite="suresh2019framework"></dt-cite></li>
</ul>

<tofro prevtext="Summary Checklist" prevlink="../checklist/" nexttext="About" nextlink="../about/"></tofro>

<h2 id="references">References</h2>

<dt-bibliography></dt-bibliography>

<script type="text/bibliography">
  
@inproceedings{buolamwini2018gender,
  title={Gender shades: Intersectional accuracy disparities in commercial gender classification},
  author={Buolamwini, Joy and Gebru, Timnit},
  booktitle={Conference on fairness, accountability and transparency},
  pages={77-91},
  year={2018},
  url={http://proceedings.mlr.press/v81/buolamwini18a.html}
}

@article{devries2019does,
  title = {Does Object Recognition Work for Everyone?},
  author = {DeVries, Terrance and Misra, Ishan and Wang, Changhan and van der Maaten, Laurens},
  journal = {arXiv preprint arXiv:1906.02659},
  year = {2019},
  url = {https://arxiv.org/abs/1906.02659}
}

@article{friedman1996bias,
  title={Bias in computer systems},
  author={Friedman, Batya and Nissenbaum, Helen},
  journal={ACM Transactions on Information Systems (TOIS)},
  volume={14},
  number={3},
  pages={330-347},
  year={1996},
  publisher={ACM},
  url={https://dl.acm.org/citation.cfm?id=230561}
}

@article{winner1980artifacts,
  title={Do artifacts have politics?},
  author={Winner, Langdon},
  journal={Daedalus},
  pages={121-136},
  year={1980},
  publisher={JSTOR},
  url={https://www.jstor.org/stable/pdf/20024652.pdf}
}

@article{jasanoff2007technologies,
  title={Technologies of humility},
  author={Jasanoff, Sheila},
  journal={Nature},
  volume={450},
  number={7166},
  pages={33},
  year={2007},
  publisher={Nature Publishing Group},
  url={http://www.imgwf.uni-luebeck.de/fileadmin/oeffentlich/JASANOFF_2007_Technologies_of_humility_Nature.pdf}
}

@article{barocas2016big,
  title={Big data's disparate impact},
  author={Barocas, Solon and Selbst, Andrew D},
  journal={Calif. L. Rev.},
  volume={104},
  pages={671},
  year={2016},
  publisher={HeinOnline},
  url={http://www.cs.yale.edu/homes/jf/BarocasSelbst.pdf}
}

@article{kleinberg2016inherent,
  title={Inherent trade-offs in the fair determination of risk scores},
  author={Kleinberg, Jon and Mullainathan, Sendhil and Raghavan, Manish},
  journal={arXiv preprint arXiv:1609.05807},
  year={2016},
  url = {https://arxiv.org/abs/1609.05807}
}

@article{barabas2017interventions,
  title={Interventions over predictions: Reframing the ethical debate for actuarial risk assessment},
  author={Barabas, Chelsea and Dinakar, Karthik and Ito, Joichi and Virza, Madars and Zittrain, Jonathan},
  journal={arXiv preprint arXiv:1712.08238},
  year={2017},
  url={https://arxiv.org/abs/1712.08238}
}

@inproceedings{verma2018fairness,
  title={Fairness definitions explained},
  author={Verma, Sahil and Rubin, Julia},
  booktitle={2018 IEEE/ACM International Workshop on Software Fairness (FairWare)},
  pages={1-7},
  year={2018},
  organization={IEEE},
  url={https://dl.acm.org/citation.cfm?id=3194776}
}

@inproceedings{selbst2019fairness,
  title={Fairness and abstraction in sociotechnical systems},
  author={Selbst, Andrew D and Boyd, Danah and Friedler, Sorelle A and Venkatasubramanian, Suresh and Vertesi, Janet},
  booktitle={Proceedings of the Conference on Fairness, Accountability, and Transparency},
  pages={59-68},
  year={2019},
  organization={ACM},
  url={http://friedler.net/papers/sts_fat2019.pdf}
}

@article{suresh2019framework,
  title={A Framework for Understanding Unintended Consequences of Machine Learning},
  author={Suresh, Harini and Guttag, John V},
  journal={arXiv preprint arXiv:1901.10002},
  year={2019},
  url={https://arxiv.org/abs/1901.10002}
}

</script>


			</div>
		</div>
		<div id="citation-hover">
		</div><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <!-- <h2 class="footer-heading"></h2> -->

    <div class="footer-col-wrapper">

      <!-- <div class="footer-col footer-col-2"> --><ul class="social-media-list"><li><a href="https://github.com/greentfrapp"><svg class="svg-icon" viewBox="0 0 16 16"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <!-- <span class="username">greentfrapp</span> --></a></li><li><a href="https://www.linkedin.com/in/sweekiat"><svg class="svg-icon" viewBox="0 0 16 16"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <!-- <span class="username">sweekiat</span> --></a></li></ul>
<!-- </div> -->

      <!-- <div class="footer-col footer-col-3">
        <p></p>
      </div> -->
    </div>

  </div>

</footer>
<div id="cookieconsent"></div>
		</div><script src="/assets/js/js.cookie.js"></script>
<script src="/assets/js/bibtexParse.js"></script>
<!-- <script src="/assets/guide/bibtex.js"></script> -->
<script src="/assets/js/bibliography.js"></script>
<script src="/assets/js/textures.js"></script>
<script src="/assets/js/d3-annotation.js"></script>
<script src="/assets/js/guide_fairnessexplorable.js"></script>
<script src="/assets/js/guide_tidbit.js"></script>
<script src="/assets/js/guide_tofro.js"></script>
<script src="/assets/js/guide_main.js"></script>
<script src="https://cdn.jsdelivr.net/npm/cookieconsent@3/build/cookieconsent.min.js" data-cfasync="false"></script>
<script src="/assets/js/cookieconsent.js"></script></body>

</html>